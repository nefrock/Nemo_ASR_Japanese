{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2021-02-04 15:18:30 experimental:27] Module <class 'nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[nltk_data] Error loading averaged_perceptron_tagger: <urlopen error\n",
      "[nltk_data]     [Errno -3] Temporary failure in name resolution>\n",
      "[nltk_data] Error loading cmudict: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n",
      "[NeMo W 2021-02-04 15:18:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-02-04 15:18:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-02-04 15:18:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-02-04 15:18:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-02-04 15:18:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-02-04 15:18:31 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2021-02-04 15:18:32 experimental:27] Module <class 'nemo.collections.asr.data.audio_to_text_dali.AudioToCharDALIDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n",
      "[NeMo W 2021-02-04 15:18:32 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pytorch_lightning as pl\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nemo.collections.asr.models import EncDecCTCModel\n",
    "from nemo.core.config import hydra_runner\n",
    "from nemo.utils import logging\n",
    "from nemo.utils.exp_manager import exp_manager\n",
    "import os\n",
    "import torch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load('conf/config-5x5_all.yaml') # 学習するときに読ませたconfigファイルからconfigを得る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2021-02-04 12:20:12 collections:173] Dataset loaded with 651505 files totalling 397.23 hours\n",
      "[NeMo I 2021-02-04 12:20:12 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-02-04 12:20:14 collections:173] Dataset loaded with 36693 files totalling 22.18 hours\n",
      "[NeMo I 2021-02-04 12:20:14 collections:174] 0 files were filtered totalling 0.00 hours\n",
      "[NeMo I 2021-02-04 12:20:14 features:235] PADDING: 16\n",
      "[NeMo I 2021-02-04 12:20:14 features:251] STFT using torch\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(**cfg['trainer'])\n",
    "asr_model = EncDecCTCModel(cfg=cfg['model'], trainer=trainer) # 重みランダムなモデルを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = torch.load('nemo_experiments/QuartzNet5x5_CSJ/2021-02-04_01-44-40/checkpoints/QuartzNet5x5_CSJ-last.ckpt') # 重みをロード\n",
    "asr_model.load_state_dict(sd['state_dict']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I05_HS.wav was recognized as: 天然記念物休の規模という学者の発言もあった\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I10_HS.wav was recognized as: 病床の母親が誰かに手紙の大質を頼む\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I15_HS.wav was recognized as:  昼前に合宿所の電話が鳴った\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I20_HS.wav was recognized as:  うこれは漁民達の日常経験である\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I25_HS.wav was recognized as:  つその水源の呼称は炒め付けられ悲鳴を上げる元気もない\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I30_HS.wav was recognized as: 最後までどうも不思議な程おいしいなと思い続けた\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I35_HS.wav was recognized as: 最も編集会議にはその方が都合が良い\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I40_HS.wav was recognized as:  みんなはそれから彼が入院中人付き合いが悪くどうも苦手だったという票をしめた\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I45_HS.wav was recognized as:  平和を望み握手し合兄弟のような気持ちで別れを告げる\n",
      "Audio in /opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I50_HS.wav was recognized as:  残酷な事実現実に目をって通り過ぎることの方がより恐ろし\n"
     ]
    }
   ],
   "source": [
    "files = ['/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I05_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I10_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I15_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I20_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I25_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I30_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I35_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I40_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I45_HS.wav', '/opt/storage/datasets/audio/japanese/JNAS/WAVES_HS/F001/PB/BF001I50_HS.wav']\n",
    "for fname, transcription in zip(files, asr_model.transcribe(paths2audio_files=files)): # 推論\n",
    "    print(f\"Audio in {fname} was recognized as: {transcription}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio in /src/CSJ/WAV/core/A01M0099.wav was recognized as:  つ すーすすー えーとまず研究の背景ですが えーこれまでの話者照合の研究では えー他の話者や録音音声による詐称につきましては えー考慮されてきましたが えー合成音声による詐称につきましては殆ど考慮されてきませんでしたしかし えー近年 えー合成技術の発達により え高頻質で えー多様な話者性を持つ音声合成音声を あ音声合成することができるようになってきたことから えー合成音声による詐称の句エ討も必要ととなってきました んでー我々はこれまでに えーテキスト指て型の話者照合システムに対して えーエ延メモに基づく音声合成システムを用いた え詐称の検討を行なってまいりました えーその結果 えー話者照合システムの登録話者による数文章の音声をこの合成システムの学習データーとして用いた場合でも えー合成音声の受例率率が非常に高くなるという結果を示しました えーしかしこれまでの え実験では話者照合システムで えースペクトル情報しか用いていなかったこともあり え合成音声の励振源として白色雑音を用いていましたそこで えー ふ本研究では話者照合システムで えーピッチ情報も利用した場合について検討を行ないましたで えーとまず えー複合性音声による えー しょ詐称の けん詐称の要素を示しま えーとまず話者照合システムでは えーに えー照合の要求がありましたら適当なテキストを生成し えー話者に提示しますね予め学習してあるその話者の音素英延メムをこの えー接続してこのテキストに対応する文一目を生成し えー入力音声から特徴パラメーターを抽出して尤度計算し えー閾値と比較することによってえ えー受理または えー棄却の判定を行ないますで えー合成音声で詐称する場合には えーまずこの えー合成の基本単位となる音素エジ延メムを登録話者の音声数文章で学習しておきますそして えー照合システムからー えーテキストが提示されましたらこの音素エジ延メムを えーこのテキストに従って接続しパラメーターを えースペクトルパラメーターよびピッチを生成して えー合成フィルターにより音声を合成しますでこの合成音声を えー話者照合システムへの入力としまえこれまでの検討では えー え話者照合システムで えー特徴パラメーターとしてスペクトルしか用いていませんでしたが えー今回はピ致情報も用いることにしましたえこれに伴い えー合成システムでも えーピッチを生成することとしましたで えーピッチを えー話者照合に用いる場合には えーピッチの有声区間と無声区間をどのように扱うかということが問題になりますこれまでに えー有声区間特の正区間を別々にモデル化しそれぞれのスコアを統合する手法や えースペクトルとピッチを別々にモデル化しそれぞれのスコアを えー統合する手法などが提案されていますが えー本研究では えースペクトルパラメーターとピッチパラメーターを え結合し えスペクトル部は連続分布ピッチ部は えー多空間分布でモデル化する手法を取りましたでちょっと前の んー発表と え重なる部分があるんですけれども  えー多空間分布でピッチをモデル化する場合にはピッチを えー有声を表わす一次元空間からの出力と無声を表わすゼロ次元空間から無出力の積 あ あ えー時系列と考えますで えー各空間は有声または無声となる確率を表わす重みと有声の場合には えーピッチの値に対する確率密度分布を持っています えーで えーこの時に えーとピッチの観測辞象は え有声の場合には有勢であることを表わすシンモルとピッチの値無声の場合には無声を表わすシンボルとなりますまた え出力確率は え有声となる有声の場合には有声となる確率と えー分布に対する尤度の積無声の場合には無声となる確率となります えこのようにすることによって えー有声ピッチの有声区間と無声区間を統一的にモデル化することができます ーで延メムでスペクトルとピッチを えー統一的にモデル化する場合には えー入力をスペクトル部とピッチ部の二つに分けスペクトル部は えー通常の連続分布ピッチ部は えー多空間分布でモデル化しますでこのヘジ円目も音素単位で学習し えー話者照合システムおよび音声合成システムで用いていますね実験条件ですが えー話者は えー男性六名女性四名の十名ですそれから えー合成システムの学習データーは えー今回は あのー学習データーとなる音声を えー十分 にゅる入手できた場合を仮定しまして え えー五十文章としました か え特徴パラメーターは えー照合システムと合成システムで異なるパラメーターを用いまして えー照合システムでは エルピーシーケプストラムおよび す えー対数基本周数 そのとそのデルター成分合成システムでは えーメルケプストラムと対数基本周波数および そのデルタ成分としましたえとそれから えー話者照合システム合成システム共に音素数は四十二で えー 一円目ムは三状態のレフトトゥーライトモデルとし えー先行後続音素や えー えアクセント型などスペクトルアピッチに変動 あ えー影響を与える変動要因を考慮したコンテキトディペンレントモデルおよび えー考慮しないコンテキトインディベンデントモデルを用いましでスペクトル部の えースペクトル部は えー単一スガアス分布でモデル化していますとー後 えー尤度の正規化法なんですが ー本研究では えー話者とテキストの同時照合を行なうこととし  え申告話者へスに対する入力音声をの正規化対数尤度は えこの式のように表わしています えーとここで えー予稿集ではこの低分の一があ抜けていますので えー訂正お願いしますで えーとこの式の中で えーこのダブリーは指定 て そしたテキストに対応するラブルレースラムダエスは申告話者のモデルそれからこの大ム字ダブ流は全ての可能なラベル列の集合からラムダオールは えー不特定話者の コンテキストインデペンデントモデルですでつまり えーこの式では えー指定したテキスと えーい申告話者のモデルに対する入力音声の尤度対数尤度から あ えー不特定話者モデルで入力音声を認識した時の第一候補となる ふラベル列の尤度を引いて えーフレーム数で正規化した形となっていますで えー結果を示します えーっとこのこの図は えー閾値を えー本人企画率と合成音声の自類率が等しくなるように設定した場合の あー誤り率である等誤り率ですで左側が えー話者照合システムで えピッチを考慮しない場合右側が考慮した場合それから上の段が合成システムで えーピッチを用いず白色雑音で励進した場合下の段はピッチ音ピッチを用いて合成した場合それから えーと黄色と緑のす えー棒グラフは えー話者照合時に えーコンテキスコンテキストインディベンレエントモデルを用いた場合赤田はコンテキストディペンデントモデルを用いた場合それから えーこの黄色と赤は え合成エジに えコンテキスイニペンデンタモデルを用いた場合で緑とはコンテキトディペンレントモデルを用いた場合です えーなお え人間の詐称者に対する えーイコールエラーレと誤り率はこの表のようになっていますでまず えー話者照合システムでピッチを考慮することによりまして えー合成時に えーピッチを生成せず白色雑音で励進した音声に対してはこのように えーと誤り率を えー大幅に低下させることができますえしかし えー合成時にもピッチを生成しピッチを用いて合成した音声に対しましては えー ほどど変らない結果とななりました それから えーコンテキストを慮ししたモデルを用いることによって えーと山率を えー低下せることができるのですが えー合成時にもコンテキストを考慮することにより えーイコールエラーレートは等誤り率は えー五十パーセントを超える結果となってしまいましたてこのことから えー まこの場合でも合成音声による箇象は十分可能であということが分かりますから えーっと正規化対数尤度の分布を示しますと えこの う図は えーっと小合時にピッチを用いた場合の図で えーこえと横軸が えー跡化した対数度縦軸は数回数を表わしていますで えーとこの表この図は えー照合システム合成システム共にコンテキト二ペンレントモデルを用いた場合でこの図はコンテキトディベンデントモデルを用いた場合それから あー小合時にコンテキトディペレントモデルを用いた場合そいからこれは合成時にもコンテキトディペンテントモデルを用いた場合ですで えーと青の分布が えー本人の音声に対する尤度の分布から赤の分布が えーピッチを用いて合成した時の音声に対する尤度の分布ですで えーとこの部分をちょっと見ていただきたいのですけれども えーこの部分見ると あのー本人の音声の分布と えー合成音声の分布が殆ど重なっていることが分かります えこのことから えー今の状態では閾値を えーな変更することによって え合成音声を効率的に棄却することは 不可能であるということが分かります えーとまとめまして えー え本研究では話者照合時に えーピッチ情報っと えー後スペクトルとピッチの変動要因考慮することによりまして えー はくじょく各色雑音で励進した音声に対しましては い えと誤り率を大幅に低下させることができたのですが えーピッチ情報を用いピッチ じゅピッチも生成した合成音声に対しましてはイコールエラーレーター五十パーセント以上となってしまいました えーこの原因としましては えー今回の実験ではスペクトルパラメーターを除いて えー合成システムと照合システムで え分析条件など殆で一緒 の条件となっていたことそれから合成システムの学習データーが文ジ文章と非常に多かったことが考えられますそこで え今後の課題としまして こ えーこの実験条件の見直しや えー他の話者照合 うー話者他の枠組みの話者照合システムに対する検討が挙げられますまた えーとー合成音声え人間が聞けば明らかに構生音生だと分かりますので えー合成音声と えー自然音声の違いを検討して えー合成音声を有効に棄却する手法 をを開発することも挙げられます以上で発表終わらします\n"
     ]
    }
   ],
   "source": [
    "files = ['/src/CSJ/WAV/core/A01M0099.wav']\n",
    "for fname, transcription in zip(files, asr_model.transcribe(paths2audio_files=files)):\n",
    "    print(f\"Audio in {fname} was recognized as: {transcription}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
